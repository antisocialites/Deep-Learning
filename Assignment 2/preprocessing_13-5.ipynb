{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c92e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "from typing import Tuple, Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_CHUNK_RE = re.compile(r'_(\\d+)\\.h5$')          # capture the trailing “…_<chunk>.h5”\n",
    "\n",
    "def _get_dataset_name(path: str) -> str:\n",
    "    \"\"\"Return the internal dataset name inside the HDF5 file.\"\"\"\n",
    "    return \"_\".join(os.path.basename(path).split('_')[:-1])\n",
    "\n",
    "def load_participant_arrays(participant_id: int, base_dir: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Concatenate all chunks for every task of one participant and return\n",
    "    four NumPy arrays in the order:\n",
    "        rest, task_motor, task_story_math, task_working_memory\n",
    "\n",
    "    Each array has shape (n_nodes, total_timepoints) or is None.\n",
    "    \"\"\"\n",
    "    # buckets: task → list[(chunk_number, matrix)]\n",
    "    buckets = {\n",
    "        \"rest\":               [],\n",
    "        \"task_motor\":         [],\n",
    "        \"task_story_math\":    [],\n",
    "        \"task_working_memory\": []\n",
    "    }\n",
    "\n",
    "    # find all relevant files, e.g. rest_105923_1.h5\n",
    "    pattern = os.path.join(base_dir, f\"*_{participant_id}_*.h5\")\n",
    "    for path in glob.glob(pattern):\n",
    "        ds_name = _get_dataset_name(path)\n",
    "\n",
    "        # identify task & chunk number\n",
    "        task = next((t for t in buckets if ds_name.startswith(t)), None)\n",
    "        if task is None:\n",
    "            continue  # skip unrecognised file\n",
    "\n",
    "        chunk_match = _CHUNK_RE.search(path)\n",
    "        chunk_num = int(chunk_match.group(1)) if chunk_match else 0\n",
    "\n",
    "        # load matrix\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            matrix = f[ds_name][()]        # (nodes, timepoints)\n",
    "\n",
    "        buckets[task].append((chunk_num, matrix))\n",
    "\n",
    "    # concatenate chunks for each task\n",
    "    out = []\n",
    "    for task, lst in buckets.items():\n",
    "        if not lst:\n",
    "            out.append(None)\n",
    "            continue\n",
    "\n",
    "        # sort by chunk number to keep temporal order\n",
    "        lst.sort(key=lambda item: item[0])\n",
    "        matrices = [m for _, m in lst]\n",
    "\n",
    "        # sanity‑check dimensionality: all chunks must share the node axis size\n",
    "        first_rows = matrices[0].shape[0]\n",
    "        if not all(mat.shape[0] == first_rows for mat in matrices):\n",
    "            raise ValueError(f\"Inconsistent node counts in {task} chunks for participant {participant_id}\")\n",
    "\n",
    "        # concat along time axis (axis=1)\n",
    "        out.append(np.concatenate(matrices, axis=1))\n",
    "\n",
    "    return tuple(out)  # (rest, motor, story_math, working_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbf97d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rest shape: (248, 284992)\n",
      "Motor shape: (248, 284992)\n",
      "Story‑Math shape: (248, 284992)\n",
      "Working‑Memory shape: (248, 284992)\n"
     ]
    }
   ],
   "source": [
    "rest_arr, motor_arr, story_math_arr, wm_arr = load_participant_arrays(105923, base_dir=\"train\")\n",
    "\n",
    "print(\"Rest shape:\",               None if rest_arr is None else rest_arr.shape)\n",
    "print(\"Motor shape:\",              None if motor_arr is None else motor_arr.shape)\n",
    "print(\"Story‑Math shape:\",         None if story_math_arr is None else story_math_arr.shape)\n",
    "print(\"Working‑Memory shape:\",     None if wm_arr is None else wm_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c22df410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(\n",
    "        arr: np.ndarray,\n",
    "        axis: int = 1,\n",
    "        eps: float = 1e-12\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Per‑row/column min‑max scaling (default: rows = nodes).\"\"\"\n",
    "    mins = arr.min(axis=axis, keepdims=True)\n",
    "    maxs = arr.max(axis=axis, keepdims=True)\n",
    "    return (arr - mins) / (maxs - mins + eps)\n",
    "\n",
    "def zscore(\n",
    "        arr: np.ndarray,\n",
    "        axis: int = 1,\n",
    "        eps: float = 1e-12\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Per‑row/column standardisation.\"\"\"\n",
    "    means = arr.mean(axis=axis, keepdims=True)\n",
    "    stds  = arr.std(axis=axis, keepdims=True)\n",
    "    return (arr - means) / (stds + eps)\n",
    "\n",
    "def downsample(\n",
    "        arr: np.ndarray,\n",
    "        *,\n",
    "        factor: Optional[int] = None,\n",
    "        target_rate: Optional[int] = None,\n",
    "        orig_rate: int = 2034,\n",
    "        axis: int = 1,\n",
    "        method: Literal[\"slice\", \"decimate\"] = \"slice\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reduce the temporal resolution.\n",
    "\n",
    "    Provide EITHER `factor`  (e.g. 16 → 1 sample every 16)\n",
    "           OR    `target_rate` (e.g. 128 Hz from 2034 Hz → factor≈16).\n",
    "\n",
    "    `method=\"slice\"` is lightning‑fast subsampling.\n",
    "    `method=\"decimate\"` (needs SciPy) applies an anti‑aliasing filter.\n",
    "    \"\"\"\n",
    "    if factor is None:\n",
    "        if target_rate is None:\n",
    "            raise ValueError(\"Specify either factor or target_rate\")\n",
    "        factor = int(round(orig_rate / target_rate))\n",
    "\n",
    "    if factor <= 1:\n",
    "        return arr  # already at (or above) target resolution\n",
    "\n",
    "    if method == \"slice\":\n",
    "        slicer = [slice(None)] * arr.ndim\n",
    "        slicer[axis] = slice(None, None, factor)\n",
    "        return arr[tuple(slicer)]\n",
    "\n",
    "    elif method == \"decimate\":\n",
    "        from scipy.signal import decimate\n",
    "        return decimate(arr, factor, axis=axis, zero_phase=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'slice' or 'decimate'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f18cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose `rest_arr` has shape (248, 35624)\n",
    "\n",
    "rest_scaled  = minmax_scale(rest_arr)          # values in [0, 1]\n",
    "rest_standardised = zscore(rest_arr)           # mean 0 / std 1\n",
    "rest_down128 = downsample(rest_arr, target_rate=128)   # ≈128 Hz via slicing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
